Все о RAG

I. Разбор статьи - Прокачиваем RAG: тестируем техники и считаем их эффективность. Часть 1
- https://habr.com/ru/articles/946888/
Автор Tuturutuw
дата публикации 14 сен 2025
Термины
как получать чанки, 
какую векторную базу использовать, 
как организовать получение релевантной информации из базы, 
выбор эмбеддера
GitHub есть прекрасный репозиторий, где автор собрал популярные RAG-техники с кодом и объяснениями. 
Репозиторий доступен по https://github.com/NirDiamant/RAG_Techniques.

датасет Natural Questions - https://github.com/google-research-datasets/natural-questions
— это датасет с реальными поисковыми запросами пользователей Google.

Метрики:
RAGAS — LLM-based оценка разных аспектов RAG-системы (faithfulness, answer relevance и т.д.),
BertScore — Оценка релевантности контекста и соответствия между ответом системы и реальным ответом.,

LLM Модель: GigaChat Lite.

В RAG (Retrieval-Augmented Generation) системах **чанки** (от англ. *chunks*) — это **фрагменты текста**, на которые разбивается исходный документ или корпус данных перед индексацией и использованием в процессе поиска (retrieval).

---

### Зачем нужны чанки?

RAG-система работает в два этапа:
1. **Retrieval (поиск)**: по запросу пользователя система ищет наиболее релевантные фрагменты из базы знаний.
2. **Generation (генерация)**: LLM использует найденные фрагменты как контекст для генерации ответа.

Но LLM не могут обработать сразу весь документ (например, книгу или длинный PDF), потому что:
- У моделей есть **ограничение на длину контекста** (например, 4096, 8192, 128K токенов).
- Длинные документы содержат **много нерелевантной информации**, что ухудшает качество ответа и увеличивает стоимость/время обработки.

Поэтому документы **разбиваются на чанки** — небольшие, логически связные куски текста, которые можно эффективно хранить, искать и подавать в модель.

---

### Как формируются чанки?

Разбиение может быть:
- **Фиксированное по количеству токенов или символов**  
  (например, каждые 512 токенов).
- **Семантическое / структурированное**  
  (с учётом абзацев, заголовков, предложений — чтобы не "разрывать" смысл).

#### Пример:
Исходный текст:
> "Париж — столица Франции. Он известен Эйфелевой башней, Лувром и кухней. Франция находится в Западной Европе..."

Чанки (размер ~20 слов):
1. "Париж — столица Франции. Он известен Эйфелевой башней, Лувром и кухней."
2. "Франция находится в Западной Европе..."

---

### Важные параметры чанков:

| Параметр                  |                   Описание                          |
|---------------------------|-----------------------------------------------------|
| **Размер чанка**          | Слишком маленький → теряется контекст; 
|                           |слишком большой → попадает нерелевантная информация. 
|                           |Типичные значения: 256–1024 токена.                  |
-----------------------------------------------------------------------------------                           
| **Перекрытие (overlap)**  | Соседние чанки могут частично пересекаться          |
|                           | (например, на 50 токенов), чтобы сохранить связность|
|                           | смысла на границах.                                 |
-----------------------------------------------------------------------------------
| **Границы разбиения**     | Лучше разбивать по абзацам, предложениям или        |
|                           | разделам, а не "вслепую" по символам.               |
-----------------------------------------------------------------------------------

### Как чанки используются в RAG?

1. Документы → разбиваются на чанки.
2. Каждый чанк → преобразуется в **эмбеддинг** (векторное представление).
3. Эмбеддинги → сохраняются в **векторной базе данных** (например, FAISS, Chroma, Pinecone).
4. При запросе пользователя:
   - Его запрос тоже превращается в эмбеддинг.
   - Система ищет **k наиболее близких чанков** по векторному сходству.
   - Эти чанки подаются в LLM как контекст для генерации ответа.

### Пример на практике:

```text
Запрос: "Что интересного в Париже?"

Система находит чанк:
"Париж — столица Франции. Он известен Эйфелевой башней, Лувром и кухней."

LLM генерирует ответ:
"В Париже вы можете посетить Эйфелеву башню, Лувр и насладиться французской кухней."
```

### Вывод:

**Чанки — это "строительные блоки" знаний в RAG**, которые позволяют эффективно искать релевантную 
информацию и подавать её в языковую модель в пределах её контекстного окна. От качества разбиения 
на чанки напрямую зависит точность и полезность ответов RAG-системы.

информация в середине обрабатывается хуже (статья Lost in the Middle: How Language Models Use Long Contexts - 
https://arxiv.org/pdf/2307.03172)
System промпт + инструкции + вопрос + контекст
На просторах Arxiv можно найти прекрасную статью (Position of Uncertainty: 
A Cross-Linguistic Study of position bias in Large Language Models - https://arxiv.org/pdf/2505.16134) 
от исследователей из ИТМО. В данной статье исследуется пристрастие LLM к 
определённым позициям релевантной информации в промпте.
https://t.me/notes_on_ml/25

Особое внимание нужно уделить выбору векторизатора, так как именно он отвечает за получение релевантной информации. 
Чаще всего FRIDA - https://habr.com/ru/companies/sberdevices/articles/909924/
или 
BGE-M3 - https://huggingface.co/BAAI/bge-m3
справляется с данной задачей на высоком уровне. 

Query Transformation
В данной технике мы пытаемся бороться с неточными запросами к системе путём их переформулирования. 
Первым делом, запрос попадает в LLM, которая должна сделать его более конкретным и полным. 
Далее этот запрос попадает в стандартную RAG систему.

Пример: "Машина Шумахера?" → "Какую машину использовал Михаэль Шумахер во время гонок в Формуле-1?"

Query Decomposition
Особую сложность RAG-система может испытывает, когда поступает составной вопрос. Например:
"Как мне попасть в офис и во сколько я должен там быть?".

HyDE (Hypothetical Document Embedding)

HyPE (Hypothetical Prompt Embeddings)
А что если не генерировать каждый раз гипотетический ответ, а в момент занесения чанков в базу данных 
сгенерировать к ним гипотетические вопросы? Именно так и работает HyPE:

Relevant Segment Extraction (RSE)
Одной из главных проблем retrieval модуля в предыдущих техниках является фиксированная длина чанка.
Работает метод следующим образом:

Генерируются все возможные сегменты. Это могут быть любые окна длинны не большей определённой максимальной длины окна.

Для каждого сегмента считается его ценность. В качестве ценности выступает комбинированная метрика, 
которая учитывает максимальное значение релевантности, среднее значение релевантности на сегменте и 
бонус за длину (чтобы отдавать предпочтение цельным кускам).
Выбор функции ценности очень влияет на качество работы. Так, если использовать простую сумму релевантностей 
(как это сделано в репозитории, который я упоминал вначале), то предпочтение будет почти всегда отдавать 
длинным сегментам со средней релевантностью (чанки под номерами 50-80), а обособленные, но релевантные 
фрагменты будут игнорироваться. Если взять просто среднее, то предпочтение будет наоборот отдаваться 
только маленьким фрагментам с высокими значениями релевантностей.
Из всех кандидатов выбираются оптимальные сегменты так, чтобы они не пересекались и в сумме не 
превышали общий лимит длины.
На последнем шаге сегменты можно слегка расширить и объединить близкие друг к другу, чтобы вместо 
множества мелких фрагментов получилось меньше, но более длинных и связных сегментов.
https://github.com/romannoff/RAG-Research - репозиторий, в котором можно ознакомиться с работой алгоритма поближе.

Semantic Chunking
Ещё одним методом, как уйти от фиксированной длины чанков, является разбиение чанков на семантически близкие блоки. 
В данной технике чанки бьются с помощью 
SemanticChunker - https://python.langchain.com/docs/how_to/semantic-chunker/
из библиотеки langchain_experimental.
Краткий принцип работы:
Разбиваем текст на предложения.
Для каждого предложения получаем его векторное представление.
Считаем косинусное расстояние между соседними предложениями. Например, между первым и вторым предложениями, между вторым и третьим, третьим и четвёртым и так далее.
Составляем распределение расстояний. Если расстояние попадает в 95-й перцентиль (эвристика и её можно менять в зависимости от желаемой длины чанков), то считаем, что данные предложения не имеют смысловой связности и между ними ставим точку разрыва.
Точки разрыва показывают, где заканчивается чанк и начинается новый.
Чанки загружаются в векторную базу данных.
Далее принцип работы не отличается от простого RAG.
Более подробнее про принцип работы можно прочитать в 
RetrievalTutorials - https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb
или 
здесь. - https://t.me/notes_on_ml/27











